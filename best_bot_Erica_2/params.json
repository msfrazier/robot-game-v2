{"action_size": 10, "activation": "relu", "epsilon_decay": 0.95, "layers": [1280, 640, 320, 160], "learning_rate": 0.001, "memory_size": 10000, "mini_batch_size": 1000, "momentum": 0.99, "output_activation": "tanh", "reg_const": 0.0, "state_size": [6]}