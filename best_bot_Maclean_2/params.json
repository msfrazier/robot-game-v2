{"action_size": 10, "activation": "relu", "epsilon_decay": 0.99, "layers": [256, 128, 64, 32], "learning_rate": 0.001, "memory_size": 10000, "mini_batch_size": 5000, "momentum": 0.99, "output_activation": "tanh", "reg_const": 0.0, "state_size": [19]}